{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('algorithm').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Load training data. \n",
    "data = pd.read_csv(\"data/credit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BAD          int64\n",
       "LOAN         int64\n",
       "MORTDUE    float64\n",
       "VALUE      float64\n",
       "REASON      object\n",
       "JOB         object\n",
       "YOJ        float64\n",
       "DEROG      float64\n",
       "DELINQ     float64\n",
       "CLAGE      float64\n",
       "NINQ       float64\n",
       "CLNO       float64\n",
       "DEBTINC    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get an idea of what the data looks like. \n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOAN</th>\n",
       "      <th>MORTDUE</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>REASON</th>\n",
       "      <th>JOB</th>\n",
       "      <th>DEROG</th>\n",
       "      <th>DELINQ</th>\n",
       "      <th>CLAGE</th>\n",
       "      <th>NINQ</th>\n",
       "      <th>property_level</th>\n",
       "      <th>STATUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1100</td>\n",
       "      <td>25860.0</td>\n",
       "      <td>39025.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.366667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>DEFAULT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1300</td>\n",
       "      <td>70053.0</td>\n",
       "      <td>68400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>121.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>DEFAULT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LOAN  MORTDUE    VALUE  REASON  JOB  DEROG  DELINQ       CLAGE  NINQ  \\\n",
       "0  1100  25860.0  39025.0       0    0    0.0     0.0   94.366667   1.0   \n",
       "1  1300  70053.0  68400.0       0    0    0.0     2.0  121.833333   0.0   \n",
       "\n",
       "   property_level   STATUS  \n",
       "0               1  DEFAULT  \n",
       "1               2  DEFAULT  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove uncorrelated factor by analyzing using corr() function\n",
    "data.drop('CLNO', axis=1, inplace=True)\n",
    "\n",
    "# Deal with missing values\n",
    "import numpy as np\n",
    "mean = data.mean()\n",
    "data.replace(np.nan, mean, inplace=True)\n",
    "\n",
    "# Unclear definition\n",
    "data.drop('DEBTINC', axis=1, inplace=True)\n",
    "\n",
    "# Uncorrelated from feature selection method using statistical method\n",
    "data.drop('YOJ', axis=1, inplace=True)\n",
    "\n",
    "# Derivate a new variable using value of property\n",
    "data['property_level'] = np.where(data['VALUE']<=66500,1,np.where(data['VALUE']<=90000,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BAD                 int64\n",
       "LOAN                int64\n",
       "MORTDUE           float64\n",
       "VALUE             float64\n",
       "REASON             object\n",
       "JOB                object\n",
       "DEROG             float64\n",
       "DELINQ            float64\n",
       "CLAGE             float64\n",
       "NINQ              float64\n",
       "property_level      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking atrributes\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BAD                 0\n",
       "LOAN                0\n",
       "MORTDUE             0\n",
       "VALUE               0\n",
       "REASON            252\n",
       "JOB               279\n",
       "DEROG               0\n",
       "DELINQ              0\n",
       "CLAGE               0\n",
       "NINQ                0\n",
       "property_level      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking NaN missing values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['REASON'] = pd.factorize(data['REASON'] )[0]\n",
    "data['JOB'] = pd.factorize(data['JOB'] )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BAD                 int64\n",
       "LOAN                int64\n",
       "MORTDUE           float64\n",
       "VALUE             float64\n",
       "REASON              int64\n",
       "JOB                 int64\n",
       "DEROG             float64\n",
       "DELINQ            float64\n",
       "CLAGE             float64\n",
       "NINQ              float64\n",
       "property_level      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking atrributes\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.BAD == 1, 'STATUS'] = 'DEFAULT'\n",
    "data.loc[data.BAD == 0, 'STATUS'] = 'PAID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove BAD\n",
    "data.drop('BAD', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LOAN                int64\n",
       "MORTDUE           float64\n",
       "VALUE             float64\n",
       "REASON              int64\n",
       "JOB                 int64\n",
       "DEROG             float64\n",
       "DELINQ            float64\n",
       "CLAGE             float64\n",
       "NINQ              float64\n",
       "property_level      int64\n",
       "STATUS             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking atrributes\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/spark.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data. \n",
    "data = spark.read.csv('data/spark.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LOAN: integer (nullable = true)\n",
      " |-- MORTDUE: double (nullable = true)\n",
      " |-- VALUE: double (nullable = true)\n",
      " |-- REASON: integer (nullable = true)\n",
      " |-- JOB: integer (nullable = true)\n",
      " |-- DEROG: double (nullable = true)\n",
      " |-- DELINQ: double (nullable = true)\n",
      " |-- CLAGE: double (nullable = true)\n",
      " |-- NINQ: double (nullable = true)\n",
      " |-- property_level: integer (nullable = true)\n",
      " |-- STATUS: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's get an idea of what the data looks like. \n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(LOAN=1100, MORTDUE=25860.0, VALUE=39025.0, REASON=0, JOB=0, DEROG=0.0, DELINQ=0.0, CLAGE=94.366666667, NINQ=1.0, property_level=1, STATUS='DEFAULT')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few things we need to do before Spark can accept the data!\n",
    "# It needs to be in the form of two columns: \"label\" and \"features\".\n",
    "\n",
    "# Import VectorAssembler and Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LOAN',\n",
       " 'MORTDUE',\n",
       " 'VALUE',\n",
       " 'REASON',\n",
       " 'JOB',\n",
       " 'DEROG',\n",
       " 'DELINQ',\n",
       " 'CLAGE',\n",
       " 'NINQ',\n",
       " 'property_level',\n",
       " 'STATUS']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's visualise the columns to help with assembly. \n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features into one vector named features.\n",
    "assembler = VectorAssembler(\n",
    "  inputCols=['LOAN',\n",
    "             'MORTDUE',\n",
    "             'VALUE',\n",
    "             'REASON',\n",
    "             'JOB',\n",
    "             'DEROG',\n",
    "             'DELINQ',\n",
    "             'CLAGE',\n",
    "             'NINQ',\n",
    "             'property_level'],\n",
    "              outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's transform the data. \n",
    "output = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the string indexer (similar to the logistic regression exercises).\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"STATUS\", outputCol=\"STATUSIndex\")\n",
    "output_fixed = indexer.fit(output).transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select the two columns we want. Features (which contains vectors), and the predictor.\n",
    "final_data = output_fixed.select(\"features\",'STATUSIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training and testing set.\n",
    "train_data,test_data = final_data.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the relevant classifiers. \n",
    "from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier,LogisticRegression\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust parameters for decision trees\n",
    "dtc1 = DecisionTreeClassifier(labelCol='STATUSIndex',featuresCol='features',maxDepth=5,impurity='gini',seed=1)\n",
    "dtc2 = DecisionTreeClassifier(labelCol='STATUSIndex',featuresCol='features',maxDepth=10,impurity='gini',seed=2)\n",
    "dtc3 = DecisionTreeClassifier(labelCol='STATUSIndex',featuresCol='features',maxDepth=10,impurity='entropy',seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models\n",
    "dtc1_model = dtc1.fit(train_data)\n",
    "dtc2_model = dtc2.fit(train_data)\n",
    "dtc3_model = dtc3.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the predictions\n",
    "dtc1_predictions = dtc1_model.transform(test_data)\n",
    "dtc2_predictions = dtc2_model.transform(test_data)\n",
    "dtc3_predictions = dtc3_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the evaluator.\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error. \n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"STATUSIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"STATUSIndex\", predictionCol=\"prediction\", metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the  evaluation\n",
    "dtc1_acc = acc_evaluator.evaluate(dtc1_predictions)\n",
    "dtc2_acc = acc_evaluator.evaluate(dtc2_predictions)\n",
    "dtc3_acc = acc_evaluator.evaluate(dtc3_predictions)\n",
    "\n",
    "dtc1_f1 = f1_evaluator.evaluate(dtc1_predictions)\n",
    "dtc2_f1 = f1_evaluator.evaluate(dtc2_predictions)\n",
    "dtc3_f1 = f1_evaluator.evaluate(dtc3_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results!\n",
      "----------------------------------------\n",
      "The first single decision tree has an accuracy of: 82.68%\n",
      "----------------------------------------\n",
      "The first single decision tree has a F1 value of: 78.51%\n",
      "----------------------------------------\n",
      "The second single decision tree has an accuracy of: 87.35%\n",
      "----------------------------------------\n",
      "The second single decision tree has a F1 value of: 86.23%\n",
      "----------------------------------------\n",
      "The third single decision tree has an accuracy of: 87.35%\n",
      "----------------------------------------\n",
      "The third single decision tree has a F1 value of: 86.29%\n"
     ]
    }
   ],
   "source": [
    "# Let's do something a bit more complex in terms of printing, just so it's formatted nicer. \n",
    "print(\"Here are the results!\")\n",
    "print('-'*40)\n",
    "print('The first single decision tree has an accuracy of: {0:2.2f}%'.format(dtc1_acc*100))\n",
    "print('-'*40)\n",
    "print('The first single decision tree has a F1 value of: {0:2.2f}%'.format(dtc1_f1*100))\n",
    "print('-'*40)\n",
    "print('The second single decision tree has an accuracy of: {0:2.2f}%'.format(dtc2_acc*100))\n",
    "print('-'*40)\n",
    "print('The second single decision tree has a F1 value of: {0:2.2f}%'.format(dtc2_f1*100))\n",
    "print('-'*40)\n",
    "print('The third single decision tree has an accuracy of: {0:2.2f}%'.format(dtc3_acc*100))\n",
    "print('-'*40)\n",
    "print('The third single decision tree has a F1 value of: {0:2.2f}%'.format(dtc3_f1*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,[0,1,2,4,5,6,7,8],[0.06321645830295264,0.05440781538455242,0.05885879600272049,0.017588143151336166,0.1362758718207821,0.4394795650788705,0.1529606001771753,0.07721275008161028])\n",
      "(10,[0,1,2,3,4,5,6,7,8],[0.14524140601021768,0.12110459811574657,0.11370163219331754,0.026346753903612626,0.04532059660517683,0.08528384853645937,0.2182960207664732,0.14290749482805407,0.10179764904094202])\n",
      "(10,[0,1,2,3,4,5,6,7,8],[0.14153887904144227,0.1236567498399055,0.08385149577093144,0.03226327706840093,0.0494570874922656,0.09278898056742525,0.20063534388140117,0.16794429043680784,0.10786389590142006])\n"
     ]
    }
   ],
   "source": [
    "# Figure out feature importances\n",
    "a = dtc1_model.featureImportances\n",
    "print(a)\n",
    "b = dtc2_model.featureImportances\n",
    "print(b)\n",
    "c = dtc3_model.featureImportances\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust parameters for random forest\n",
    "rfc1 = RandomForestClassifier(labelCol='STATUSIndex',featuresCol='features',numTrees=20,subsamplingRate=1,seed=4)\n",
    "rfc2 = RandomForestClassifier(labelCol='STATUSIndex',featuresCol='features',numTrees=20,subsamplingRate=0.8,seed=5)\n",
    "rfc3 = RandomForestClassifier(labelCol='STATUSIndex',featuresCol='features',numTrees=35,subsamplingRate=1,seed=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models\n",
    "rfc1_model = rfc1.fit(train_data)\n",
    "rfc2_model = rfc2.fit(train_data)\n",
    "rfc3_model = rfc3.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the predictions\n",
    "rfc1_predictions = rfc1_model.transform(test_data)\n",
    "rfc2_predictions = rfc2_model.transform(test_data)\n",
    "rfc3_predictions = rfc3_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the  evaluation\n",
    "rfc1_acc = acc_evaluator.evaluate(rfc1_predictions)\n",
    "rfc2_acc = acc_evaluator.evaluate(rfc2_predictions)\n",
    "rfc3_acc = acc_evaluator.evaluate(rfc3_predictions)\n",
    "\n",
    "rfc1_f1 = f1_evaluator.evaluate(rfc1_predictions)\n",
    "rfc2_f1 = f1_evaluator.evaluate(rfc2_predictions)\n",
    "rfc3_f1 = f1_evaluator.evaluate(rfc3_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results!\n",
      "----------------------------------------\n",
      "The first random forest has an accuracy of: 84.13%\n",
      "----------------------------------------\n",
      "The first random forest has a F1 value of: 80.88%\n",
      "----------------------------------------\n",
      "The second random forest has an accuracy of: 84.80%\n",
      "----------------------------------------\n",
      "The second random forest has a F1 value of: 81.70%\n",
      "----------------------------------------\n",
      "The third random forest has an accuracy of: 84.55%\n",
      "----------------------------------------\n",
      "The third random forest has a F1 value of: 81.17%\n"
     ]
    }
   ],
   "source": [
    "# Let's do something a bit more complex in terms of printing, just so it's formatted nicer. \n",
    "print(\"Here are the results!\")\n",
    "print('-'*40)\n",
    "print('The first random forest has an accuracy of: {0:2.2f}%'.format(rfc1_acc*100))\n",
    "print('-'*40)\n",
    "print('The first random forest has a F1 value of: {0:2.2f}%'.format(rfc1_f1*100))\n",
    "print('-'*40)\n",
    "print('The second random forest has an accuracy of: {0:2.2f}%'.format(rfc2_acc*100))\n",
    "print('-'*40)\n",
    "print('The second random forest has a F1 value of: {0:2.2f}%'.format(rfc2_f1*100))\n",
    "print('-'*40)\n",
    "print('The third random forest has an accuracy of: {0:2.2f}%'.format(rfc3_acc*100))\n",
    "print('-'*40)\n",
    "print('The third random forest has a F1 value of: {0:2.2f}%'.format(rfc3_f1*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,[0,1,2,3,4,5,6,7,8,9],[0.12302124650498765,0.045848350060963036,0.04909976567773711,0.008129576406415564,0.01574876481622397,0.21586583477082577,0.35071602447516526,0.12869292539899196,0.058833583573123883,0.004043928315565772])\n",
      "(10,[0,1,2,3,4,5,6,7,8,9],[0.12567880813887544,0.056160576930143466,0.04298737620119368,0.009520926301476117,0.022536995622743118,0.18506613090500545,0.36215291291166596,0.10777506162631204,0.08330191745553163,0.004819293907053056])\n",
      "(10,[0,1,2,3,4,5,6,7,8,9],[0.09770477974731251,0.047537020711648996,0.049981479227129595,0.008838565930629417,0.01654866910538082,0.1963237330377655,0.3763208781231503,0.12175408127488985,0.0817610306725758,0.0032297621695171258])\n"
     ]
    }
   ],
   "source": [
    "# Figure out feature importances\n",
    "a = rfc1_model.featureImportances\n",
    "print(a)\n",
    "b = rfc2_model.featureImportances\n",
    "print(b)\n",
    "c = rfc3_model.featureImportances\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust parameters for logistic regression\n",
    "log1 = LogisticRegression(featuresCol='features',labelCol='STATUSIndex',family=\"auto\")\n",
    "log2 = LogisticRegression(featuresCol='features',labelCol='STATUSIndex',family=\"binomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models\n",
    "log1_model = log1.fit(train_data)\n",
    "log2_model = log2.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the predictions\n",
    "log1_predictions = log1_model.transform(test_data)\n",
    "log2_predictions = log2_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the  evaluation\n",
    "log1_acc = acc_evaluator.evaluate(log1_predictions)\n",
    "log2_acc = acc_evaluator.evaluate(log2_predictions)\n",
    "\n",
    "log1_f1 = f1_evaluator.evaluate(log1_predictions)\n",
    "log2_f1 = f1_evaluator.evaluate(log2_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results!\n",
      "----------------------------------------\n",
      "The first logistic regression has an accuracy of: 82.51%\n",
      "----------------------------------------\n",
      "The first logistic regression has a F1 value of: 78.76%\n",
      "----------------------------------------\n",
      "The second logistic regression has an accuracy of: 82.51%\n",
      "----------------------------------------\n",
      "The second logistic regression has a F1 value of: 78.76%\n"
     ]
    }
   ],
   "source": [
    "# Let's do something a bit more complex in terms of printing, just so it's formatted nicer. \n",
    "print(\"Here are the results!\")\n",
    "print('-'*40)\n",
    "print('The first logistic regression has an accuracy of: {0:2.2f}%'.format(log1_acc*100))\n",
    "print('-'*40)\n",
    "print('The first logistic regression has a F1 value of: {0:2.2f}%'.format(log1_f1*100))\n",
    "print('-'*40)\n",
    "print('The second logistic regression has an accuracy of: {0:2.2f}%'.format(log2_acc*100))\n",
    "print('-'*40)\n",
    "print('The second logistic regression has a F1 value of: {0:2.2f}%'.format(log2_f1*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegressionModel' object has no attribute 'featureImportances'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-0d4e5c709449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Figure out feature importances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog1_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatureImportances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog2_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatureImportances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegressionModel' object has no attribute 'featureImportances'"
     ]
    }
   ],
   "source": [
    "# Figure out feature importances\n",
    "a = log1_model.featureImportances\n",
    "print(a)\n",
    "b = log2_model.featureImportances\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
